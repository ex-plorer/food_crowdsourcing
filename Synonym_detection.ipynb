{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synonyms detection\n",
    "    Try to normalise words with different spellings to a single, common spelling to ensure that equivalent tokens receive similar representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20180206'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import myfunctions as f\n",
    "f.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system_info: 2.7.14 |Anaconda custom (64-bit)| (default, Oct 27 2017, 18:21:12) \n",
      "[GCC 7.2.0]\n",
      "path_info: /home/yueliu/Desktop/workspace_yue/Documentation_201712\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print \"system_info: %s\"%sys.version\n",
    "# current working directory\n",
    "import os\n",
    "print \"path_info: %s\"%os.getcwd()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrived: all_food_vendor__lemma__20180122.p\n",
      "Got 333572 non-drink/dessert food-vendor records\n",
      "Got 98238 unique non-drink/dessert food \n"
     ]
    }
   ],
   "source": [
    "file_name = \"all_food_vendor__lemma__20180122.p\"\n",
    "df = f.retrive_file(file_name)\n",
    "print (\"Got %d non-drink/dessert food-vendor records\" % df.shape[0])\n",
    "print (\"Got %d unique non-drink/dessert food \" % \n",
    "       df[\"clean_name\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>food_name</th>\n",
       "      <th>vendor_name</th>\n",
       "      <th>locs</th>\n",
       "      <th>clean_name</th>\n",
       "      <th>lemma_name</th>\n",
       "      <th>lemma_name_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\u0003Japanese Pickle Roll (6 pcs)</td>\n",
       "      <td>Sen of Japan</td>\n",
       "      <td>[(2017-07-03T17:55:01.668428, deliveroo/food/7...</td>\n",
       "      <td>japanese pickle roll</td>\n",
       "      <td>japanese pickle roll</td>\n",
       "      <td>japanese pickle roll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\\t HK Kailan with Oyster Sauce\\t</td>\n",
       "      <td>Wee Nam Kee Hainanese Chicken Rice - Boon Lay</td>\n",
       "      <td>[(2018-01-01T04:00:05.703635, deliveroo/food/A...</td>\n",
       "      <td>kailan with oyster sauce</td>\n",
       "      <td>kailan with oyster sauce</td>\n",
       "      <td>kailan with oyster sauce</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          food_name  \\\n",
       "4     \u0003Japanese Pickle Roll (6 pcs)   \n",
       "5  \\t HK Kailan with Oyster Sauce\\t   \n",
       "\n",
       "                                     vendor_name  \\\n",
       "4                                   Sen of Japan   \n",
       "5  Wee Nam Kee Hainanese Chicken Rice - Boon Lay   \n",
       "\n",
       "                                                locs  \\\n",
       "4  [(2017-07-03T17:55:01.668428, deliveroo/food/7...   \n",
       "5  [(2018-01-01T04:00:05.703635, deliveroo/food/A...   \n",
       "\n",
       "                 clean_name                lemma_name  \\\n",
       "4      japanese pickle roll      japanese pickle roll   \n",
       "5  kailan with oyster sauce  kailan with oyster sauce   \n",
       "\n",
       "               lemma_name_2  \n",
       "4      japanese pickle roll  \n",
       "5  kailan with oyster sauce  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrived: /home/yueliu/Desktop/workspace_yue/syn_token__20170926.p\n",
      "number of syn tokens: 3764\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['po',\n",
       " 'bao',\n",
       " 'bau',\n",
       " 'bun',\n",
       " 'pao',\n",
       " 'pau',\n",
       " u'pav',\n",
       " 'paw',\n",
       " 'poh',\n",
       " 'pow',\n",
       " u'buns',\n",
       " u'paws',\n",
       " 'wrap',\n",
       " 'bread',\n",
       " u'wraps',\n",
       " 'batter',\n",
       " u'breads',\n",
       " 'man do',\n",
       " 'breaded',\n",
       " u'man dou',\n",
       " u'man tao',\n",
       " u'man tau',\n",
       " 'man tou',\n",
       " u'man tow',\n",
       " 'wrapped',\n",
       " 'battered',\n",
       " u'man doing']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" view past synonyms: lists of lists \"\"\"\n",
    "def retrive_syn_tokens():\n",
    "    import os\n",
    "    import pickle\n",
    "    prefix = \"syn_token\"  \n",
    "    original_path = \"/home/yueliu/Desktop/workspace_yue\"\n",
    "    date_list = list(set([f.split(\"__\")[-1].replace(\".p\", \"\") \n",
    "         for f in os.listdir(original_path) if (f.endswith(\".p\") and (prefix) in f)]))\n",
    "    recent_date = sorted(date_list)[-1]\n",
    "    file_name = os.path.join(original_path,\"__\".join([prefix, recent_date]) + \".p\")\n",
    "    with open(file_name, 'rb') as pfile:\n",
    "        retrived = pickle.load(pfile)       \n",
    "    print (\"retrived: %s\" % file_name)\n",
    "    print \"number of syn tokens: %d\"%len(retrived)\n",
    "    return retrived\n",
    "\n",
    "syn_tokens = retrive_syn_tokens()\n",
    "syn_tokens[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct new synonym sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 training epochs so far.\n",
      "16,239 terms in the food2vec vocabulary.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences = [l.split() for l in df[\"lemma_name\"].tolist()]\n",
    "word2vec_filepath = os.path.join(os.getcwd(), 'word2vec_model_all')\n",
    "\n",
    "# train the model, Commonly, used min_count=20, ,use min_count=1 to find spelling errors\n",
    "if 1==0:\n",
    "    # initiate the model and perform the first epoch of training\n",
    "    food2vec = Word2Vec(sentences, size=100, window=2,\n",
    "                        min_count=1, sg=1, workers=4)\n",
    "    food2vec.save(word2vec_filepath)\n",
    "    # perform another 11 epochs of training\n",
    "    for i in range(1,12):\n",
    "        food2vec.train(sentences)\n",
    "        food2vec.save(word2vec_filepath)\n",
    "        \n",
    "# load the finished model from disk\n",
    "food2vec = Word2Vec.load(word2vec_filepath)\n",
    "food2vec.init_sims()\n",
    "\n",
    "print u'{} training epochs so far.'.format(food2vec.train_count)\n",
    "print u'{:,} terms in the food2vec vocabulary.'.format(len(food2vec.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16221\n"
     ]
    }
   ],
   "source": [
    "# build a dictionary of terms and term counts from the food2vec model vocabulary\n",
    "vocab_freq = {term:voc.count for term, voc in food2vec.vocab.iteritems()}\n",
    "\n",
    "# build a list of the terms, integer indices,\n",
    "# and term counts from the food2vec model vocabulary\n",
    "import pandas as pd\n",
    "ordered_vocab = [(term, voc.index, voc.count) for term, voc in food2vec.vocab.iteritems()]\n",
    "# sort by the term counts, so the most common terms appear first\n",
    "ordered_vocab = sorted(ordered_vocab, key=lambda (term, index, count): -count)\n",
    "# unzip the terms, integer indices, and counts into separate lists\n",
    "ordered_terms, term_indices, term_counts = zip(*ordered_vocab)\n",
    "# create a DataFrame with the food2vec vectors as data, and the terms as row labels\n",
    "word_vectors = pd.DataFrame(food2vec.syn0norm[term_indices, :],index=ordered_terms)\n",
    "\n",
    "# select a subset of words considered for pairs\n",
    "all_words = list(word_vectors.index)\n",
    "word_subset = [ w for w in all_words if (len(w)>1 and w!='-PRON-')]\n",
    "print len(word_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find misspellings\n",
    "    Check for tokens with the similar embeddings & similar spellings\n",
    "Conditions:\n",
    "> 1. rank: topn = round(16239*0.001,1)\n",
    "2. similarity: sim_cutoff=np.percentile(sims, 90)\n",
    "3. levenstein distance or (containment & larger levenstein distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DescribeResult(nobs=263705121, minmax=(-0.47539659805241563, 1.0000000000000016), mean=0.38247606905380382, variance=0.036665012092387969, skewness=-0.4478078218206751, kurtosis=0.41555732927058076)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEDCAYAAAAVyO4LAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f708af2d450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "## find pair-wise similarities of all tokens\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "sims = cosine_similarity(word_vectors.values.tolist())\n",
    "\n",
    "# print description\n",
    "print stats.describe(sims.flatten())\n",
    "n, bins, patches = plt.hist(sims.flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sims = pd.DataFrame(sims, index=word_vectors.index, columns=word_vectors.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain pairs to measure the levenstein distance\n",
    "import numpy as np\n",
    "def potential_pairs(_df_sims=df_sims, _word_subset=word_subset, \n",
    "                    sim_cutoff=np.percentile(sims, 90), rank_cutoff=50):\n",
    "    pair_ranks = {}\n",
    "    for w in _word_subset: \n",
    "        vals = _df_sims[w]\n",
    "        ranks = vals.rank(ascending=False)\n",
    "        for w1 in _word_subset:\n",
    "            v = vals[w1]\n",
    "            if (w!=w1) & (v > sim_cutoff):\n",
    "                r = int(ranks[w1])\n",
    "                if r < rank_cutoff:\n",
    "                    pair_ranks.update({(w, w1):r})\n",
    "    print \"number of pairs considered: %d\"%len(pair_ranks)\n",
    "    return pair_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of pairs considered: 636071\n",
      "CPU times: user 1h 10min 50s, sys: 23.5 s, total: 1h 11min 13s\n",
      "Wall time: 1h 10min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pair_ranks = potential_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((u'rench', u'clifshot'), 17),\n",
       " ((u'mezashi', u'amristari'), 12),\n",
       " ((u'suicide', u'chickren'), 7),\n",
       " ((u'option', u'masama'), 4),\n",
       " ((u'bhujia', u'chatpate'), 19),\n",
       " ((u'dom', u'burgudy'), 13),\n",
       " ((u'licenciado', u'viura'), 32),\n",
       " ((u'calon', u'estaphe'), 38),\n",
       " ((u'milano', u'bari'), 6),\n",
       " ((u'brunello', u'castellani'), 27),\n",
       " ((u'cencaru', u'bandeng'), 36),\n",
       " ((u'lachna', u'lacha'), 6),\n",
       " ((u'stirlion', u'nz'), 30),\n",
       " ((u'temptation', u'seitan'), 49),\n",
       " ((u'tex', u'jai'), 48),\n",
       " ((u'heartshape', u'twiscuit'), 23),\n",
       " ((u'pendang', u'nena'), 23),\n",
       " ((u'khew', u'wann'), 18),\n",
       " ((u'taak', u'krop'), 29),\n",
       " ((u'naw', u'moung'), 24),\n",
       " ((u'carlsberg', u'crazin'), 30),\n",
       " ((u'khasi', u'urundai'), 30),\n",
       " ((u'barbazul', u'valdemoreda'), 41),\n",
       " ((u'kirmizi', u'kelle'), 25),\n",
       " ((u'thing', u'buff'), 14),\n",
       " ((u'trufful', u'kanpyo'), 30),\n",
       " ((u'vieille', u'motte'), 18),\n",
       " ((u'chapa', u'sieben'), 14),\n",
       " ((u'ite', u'clipper'), 36),\n",
       " ((u'pyramid', u'kichadi'), 27)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print out some potential pairs \n",
    "[(k, pair_ranks[k]) for k in pair_ranks.keys()[:30]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def highly_ranked(token_a, token_b, topn=50, _pair_ranks=pair_ranks):\n",
    "    try:\n",
    "        r = _pair_ranks[(token_a, token_b)]\n",
    "        if r < topn:\n",
    "            return True\n",
    "    except:\n",
    "        try:\n",
    "            r = _pair_ranks[(token_b, token_a)]\n",
    "            if r < topn:\n",
    "                return True\n",
    "        except:\n",
    "            return False\n",
    "# if token_a contains token_b, or the reverse\n",
    "def string_contain(token_a, token_b):\n",
    "    if (token_a in token_b) | (token_b in token_a):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Levenstein distance (efficient implementation via numpy), from Wikipedia\n",
    "def levenshtein(source, target):\n",
    "    import numpy as np\n",
    "    if len(source) < len(target):\n",
    "        return levenshtein(target, source)\n",
    "\n",
    "    # So now we have len(source) >= len(target).\n",
    "    if len(target) == 0:\n",
    "        return len(source)\n",
    "\n",
    "    # We call tuple() to force strings to be used as sequences\n",
    "    # ('c', 'a', 't', 's') - numpy uses them as values by default.\n",
    "    source = np.array(tuple(source))\n",
    "    target = np.array(tuple(target))\n",
    "\n",
    "    # We use a dynamic programming algorithm, but with the\n",
    "    # added optimization that we only need the last two rows\n",
    "    # of the matrix.\n",
    "    previous_row = np.arange(target.size + 1)\n",
    "    for s in source:\n",
    "        # Insertion (target grows longer than source):\n",
    "        current_row = previous_row + 1\n",
    "\n",
    "        # Substitution or matching:\n",
    "        # Target and source items are aligned, and either\n",
    "        # are different (cost of 1), or are the same (cost of 0).\n",
    "        current_row[1:] = np.minimum(\n",
    "                current_row[1:],\n",
    "                np.add(previous_row[:-1], target != s))\n",
    "\n",
    "        # Deletion (target grows shorter than source):\n",
    "        current_row[1:] = np.minimum(\n",
    "                current_row[1:],\n",
    "                current_row[0:-1] + 1)\n",
    "\n",
    "        previous_row = current_row\n",
    "\n",
    "    return previous_row[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "513632\n",
      "[u'aa', u'ahi', 2]\n",
      "CPU times: user 46.5 s, sys: 208 ms, total: 46.7 s\n",
      "Wall time: 46.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\" compute levenstein distances\"\"\"\n",
    "pairs = pair_ranks.keys()\n",
    "unique_pairs = sorted(list(set([tuple(sorted(p)) for p in pairs])))\n",
    "dist = [[p[0],p[1],levenshtein(p[0], p[1])]  for p in unique_pairs]\n",
    "print len(dist)\n",
    "print dist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 924 ms, sys: 68 ms, total: 992 ms\n",
      "Wall time: 933 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\" \n",
    "1. rank(token_a, token_b) among the top 0.1% of all tokens\n",
    "2. levenstein(token_a, token_b) <= 1\n",
    "OR: levenstein(token_a, token_b) <= 2 & token_a contains token_b\n",
    "=> get word pairs\n",
    "=> get word synsets\n",
    "\"\"\"\n",
    "from copy import copy\n",
    "max_dist = 1\n",
    "max_dist2 = 4\n",
    "topn = round(16239*0.001,1)\n",
    "result_pairs = []\n",
    "for d in dist:\n",
    "    if highly_ranked(d[0], d[1], topn):\n",
    "        d1 = copy(d)\n",
    "        if string_contain(d[0], d[1]):\n",
    "            d1.append(\"True\")\n",
    "        if d1[2] <= max_dist or (len(d1)>3 and d1[2] <= max_dist2):\n",
    "            result_pairs.append(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1446\n"
     ]
    }
   ],
   "source": [
    "print max_dist\n",
    "print len(result_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the syntoken pairs into lists of lists\n",
    "def add_new(pair_lst, vocab_freq):\n",
    "    from copy import deepcopy\n",
    "    \"\"\" the words in pairs \"\"\"\n",
    "    updated = [list(lst) for lst in syn_tokens]\n",
    "    updated.extend([p[:2] for p in pair_lst])\n",
    "    # words not in vocab_freq is assigned freq = 0\n",
    "    vocab_freq.update({token:0 for token \n",
    "                       in set([item for sub in updated for item in sub]\n",
    "                             ).difference(set(vocab_freq.keys()))})\n",
    "    \"\"\" merge pairs to lists, via graph \"\"\"\n",
    "    from collections import defaultdict\n",
    "    def connected_components(lists):\n",
    "        neighbors = defaultdict(set)\n",
    "        seen = set()\n",
    "        for each in lists:\n",
    "            for item in each:\n",
    "                neighbors[item].update(each)\n",
    "        def component(node, neighbors=neighbors, seen=seen, see=seen.add):\n",
    "            nodes = set([node])\n",
    "            next_node = nodes.pop\n",
    "            while nodes:\n",
    "                node = next_node()\n",
    "                see(node)\n",
    "                nodes |= neighbors[node] - seen\n",
    "                yield node\n",
    "        for node in neighbors:\n",
    "            if node not in seen:\n",
    "                yield sorted(component(node))    \n",
    "    \"\"\" check if new token sets can be merged \"\"\"\n",
    "    temp = list(connected_components(updated))\n",
    "    # higher freq, shorter spelling in front\n",
    "    def sort_synset(l):\n",
    "        l_withspace = list(set([v for v in l if \" \" in v]))\n",
    "        l_wospace = list(set([v for v in l if \" \" not in v]))\n",
    "        combined = sorted(l_withspace ,key=lambda x: (-vocab_freq[x], len(x), x)\n",
    "                   ) + sorted(l_wospace ,key=lambda x: (-vocab_freq[x], len(x), x))\n",
    "        return combined\n",
    "        \n",
    "    temp = sorted([sort_synset(lst) for lst in temp])\n",
    "    print \"number of syn-token sets change from %d to %d\"%(len(syn_tokens), len(temp))    \n",
    "#     \"\"\" print out updated synsets \"\"\"      \n",
    "#     lst_check = [l for l in temp if l not in syn_tokens] # in updated,  not in temp\n",
    "#     print \"number of syn-token sets to be checked: %d\"%len(lst_check)\n",
    "#     def look_for_token(ele, lst):\n",
    "#         index = []\n",
    "#         for i in range(len(lst)):\n",
    "#             if ele in lst[i]:\n",
    "#                 index.append(i)\n",
    "#         return index\n",
    "#     for lst in lst_check:        # check merged syntokens\n",
    "#         result = []\n",
    "#         for ele in lst:\n",
    "#             result.extend(look_for_token(ele,syn_tokens))\n",
    "#         print \"\"\n",
    "#         print lst\n",
    "#         for i in sorted(list(set(result))):\n",
    "#             print i\n",
    "#             print syn_tokens[i]\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of syn-token sets change from 0 to 962\n"
     ]
    }
   ],
   "source": [
    "# start fresh\n",
    "syn_tokens = []\n",
    "syn_tokens = add_new(result_pairs, vocab_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'aba', u'abba'], [u'absolut', u'absolute'], [u'adige', u'adigo']]\n"
     ]
    }
   ],
   "source": [
    "print syn_tokens[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_syn_tokens(syn_tokens):\n",
    "    import pickle\n",
    "    file_name = \"__\".join([\"syn_token\", f.today()]) + \".p\"  \n",
    "    with open(file_name, 'wb') as pfile:\n",
    "        pickle.dump(syn_tokens, pfile)\n",
    "    print \"saved: %s\"%file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# itentify syntoken sets via a token\n",
    "def find_syntoken_set(s, syn_tokens=syn_tokens):\n",
    "    for lst in syn_tokens:\n",
    "        if s in lst:\n",
    "            print lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'tom', u'yum', u'yom']\n"
     ]
    }
   ],
   "source": [
    "find_syntoken_set(\"tom\", syn_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# update a wrong synset with a correct version\n",
    "def replace_syntoken_set(lst1, lst2, _syn_tokens, _vocab_freq=vocab_freq):\n",
    "    temp = [x for x in _syn_tokens if x != lst1] +[lst2]\n",
    "    # update vocab_freq\n",
    "    _vocab_freq.update({token:0 for token  in set([item for sub in temp for \n",
    "                                                   item in sub]\n",
    "                             ).difference(set(_vocab_freq.keys()))})\n",
    "    # higher freq, shorter spelling in front\n",
    "    temp = sorted([sorted(list(set(lst)) ,key=lambda x: (-_vocab_freq[x], len(x), x))\n",
    "            for lst in temp])\n",
    "    print \"number of syn-token sets change from %d to %d\"%(len(_syn_tokens), len(temp))    \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of syn-token sets change from 962 to 962\n"
     ]
    }
   ],
   "source": [
    "syn_tokens = replace_syntoken_set([u'tom', u'yum', u'yom'], [u'yum', u'yom'], \n",
    "                                  syn_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'yum', u'yom']\n"
     ]
    }
   ],
   "source": [
    "find_syntoken_set(\"yum\", syn_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find incorrect spaces\n",
    "    Check for possible connected tokens (phrases)\n",
    "Conditions:\n",
    "> 1. check for words commonly appear together to be phrases (2,3,4-gram)\n",
    "2. see whether phrases(2,3,4-gram) exist in vocab by removing \"_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "unigram_sentences  = [l.split() for l in df[\"lemma_name\"].tolist()]\n",
    "bigram_model = Phrases(unigram_sentences)\n",
    "bigram_sentences = []\n",
    "for unigram_sentence in unigram_sentences:\n",
    "    bigram_sentences.append(bigram_model[unigram_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numer of bigram tokens: 3778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'-PRON-_own',\n",
       " u'-PRON-_tiao',\n",
       " u'-PRON-_would',\n",
       " u'aburi_ika',\n",
       " u'aburi_maguro',\n",
       " u'aburi_mekajiki',\n",
       " u'aburi_nama',\n",
       " u'acai_superfruit',\n",
       " u'adai_avial',\n",
       " u'adana_kebab',\n",
       " u'adelaide_hill',\n",
       " u'age_dashi',\n",
       " u'aged_cuttlefish',\n",
       " u'aged_tempranillo',\n",
       " u'aged_vinegar',\n",
       " u'agedashi_tofu',\n",
       " u'aglio_oilo',\n",
       " u'aglio_olio',\n",
       " u'agnello_alla',\n",
       " u'agnello_grigliato']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_tokens = set([w for sentence in bigram_sentences for w in sentence])\n",
    "bigram_tokens = sorted(list(w for w in bigram_tokens if \"_\" in w))\n",
    "print \"numer of bigram tokens: %d\"%len(bigram_tokens)\n",
    "bigram_tokens[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numer of multigram tokens: 1654\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'-PRON-_tiao_biscuit_rojak',\n",
       " u'aburi_ika_sushi',\n",
       " u'aburi_mekajiki_sushi',\n",
       " u'aburi_nama_hotate_sushi',\n",
       " u'adelaide_hill_south_australia',\n",
       " u'aglio_olio_peperoncino',\n",
       " u'aglio_olio_pepperoncino',\n",
       " u'ah_kon_ginger',\n",
       " u'ah_ma_mian',\n",
       " u'ah_por_ginger',\n",
       " u'ai_frutti_di_mare',\n",
       " u'ai_quattro_formaggi',\n",
       " u'ai_yu_jelly',\n",
       " u'air_fly_grass_feed',\n",
       " u'airflown_prime_ribeye_steak',\n",
       " u'airflown_prime_striploin_steak',\n",
       " u'ajwaini_machli_tikka',\n",
       " u'aka_ebi_aburi',\n",
       " u'aka_ebi_sushi',\n",
       " u'al_funghi_porcini']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multigram_model = Phrases(bigram_sentences)\n",
    "multigram_sentences = []\n",
    "for bigram_sentence in bigram_sentences:\n",
    "    multigram_sentences.append(multigram_model[bigram_sentence])\n",
    "multigram_tokens = set([w for sentence in multigram_sentences for w in sentence])\n",
    "multigram_tokens = sorted(list(w for w in multigram_tokens if len(w.split(\"_\"))>2))\n",
    "print \"numer of multigram tokens: %d\"%len(multigram_tokens)\n",
    "multigram_tokens[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of misspellings identified: 269\n"
     ]
    }
   ],
   "source": [
    "result=[]\n",
    "for w in bigram_tokens + multigram_tokens:\n",
    "    word_trasformed = w.replace(\"_\", \"\")\n",
    "    if word_trasformed in vocab_freq.keys():\n",
    "        result.append(w)\n",
    "print \"number of misspellings identified: %d\"%len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'age_dashi',\n",
       " u'ai_yu',\n",
       " u'aka_ebi',\n",
       " u'all_aragosta',\n",
       " u'aloe_vera',\n",
       " u'baba_ganoush',\n",
       " u'baby_back',\n",
       " u'back_rib',\n",
       " u'bai_l',\n",
       " u'bara_chirashi',\n",
       " u'bean_curd',\n",
       " u'bean_sprout',\n",
       " u'bee_hoon',\n",
       " u'bi_bim',\n",
       " u'bi_hun',\n",
       " u'bird_nest',\n",
       " u'bitter_gourd',\n",
       " u'bitter_ground',\n",
       " u'black_pepper',\n",
       " u'blue_berry']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of syn-token sets change from 962 to 1168\n"
     ]
    }
   ],
   "source": [
    "syn_tokens = add_new([[w.replace(\"_\", \"\"), w.replace(\"_\", \" \")] for w in result]\n",
    "                     , vocab_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: syn_token__20180207.p\n"
     ]
    }
   ],
   "source": [
    "save_syn_tokens(syn_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name processing with synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some multi-word phrases may be in the same synset as their components\n",
    "> 1. print out these phrases\n",
    "2. manually check synsets contraining phrases, split the synsets into different synsets by replacing synset with a partial version and add additional synset if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "# print all synset with phrase contain word\n",
    "check = []\n",
    "for l in syn_tokens:\n",
    "    ind = False\n",
    "    l_withspace = list(set([v for v in l if \" \" in v]))\n",
    "    l_wospace = list(set([v for v in l if \" \" not in v]))\n",
    "    for p in l_withspace:\n",
    "        ps = p.split()\n",
    "        for a in l_wospace:\n",
    "            if a in ps:\n",
    "                ind = True\n",
    "    if ind:\n",
    "        check.append(l)\n",
    "print len(check)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'age dashi', u'agedashi', u'dashi', u'dashitofu'],\n",
       " [u'back rib', u'back', u'backrib', u'backribs'],\n",
       " [u'bean curd', u'beancurd', u'curd', u'curb'],\n",
       " [u'bee hoon', u'hoon', u'bee', u'beehoon', u'meehoon'],\n",
       " [u'bi bim', u'bi', u'bibim', u'bim', u'hebi'],\n",
       " [u'chirashi don', u'chirashi', u'shirashi', u'chirashidon'],\n",
       " [u'clay pot', u'claypot', u'clay'],\n",
       " [u'cous cous', u'cous', u'couscous'],\n",
       " [u'criss cut', u'criss', u'cross', u'cris', u'crisscut'],\n",
       " [u'dim sum', u'dim', u'dimsum'],\n",
       " [u'fai dang', u'fai', u'faidang'],\n",
       " [u'fu rong', u'fu yong', u'yong', u'fuyong', u'furong'],\n",
       " [u'fugu mirin', u'mirin', u'fugumirin'],\n",
       " [u'hara bhara',\n",
       "  u'bara',\n",
       "  u'hara',\n",
       "  u'bhara',\n",
       "  u'harasu',\n",
       "  u'harabhara',\n",
       "  u'harasume',\n",
       "  u'bharaa'],\n",
       " [u'hor fun', u'fun', u'horfun'],\n",
       " [u'ikan bili', u'bili', u'ikambili', u'ikanbili'],\n",
       " [u'jing du', u'jing', u'jingdu'],\n",
       " [u'kai lan', u'lan', u'kailan'],\n",
       " [u'kang kong', u'kang', u'kangkong', u'kangkung', u'kan'],\n",
       " [u'kra tiem', u'kra', u'kratiem', u'krai', u'kratiam', u'takkrai'],\n",
       " [u'lily bulb', u'lily', u'lilybulb'],\n",
       " [u'mc laren', u'mc', u'laren', u'clare', u'mclaren', u'mclren'],\n",
       " [u'mentai yaki', u'mentai', u'mentaiyaki'],\n",
       " [u'nor mai', u'nor', u'normai'],\n",
       " [u'phu nim', u'nim', u'phunim'],\n",
       " [u'rogan josh',\n",
       "  u'rogan',\n",
       "  u'roganjosh',\n",
       "  u'roghan',\n",
       "  u'rugan',\n",
       "  u'rogam',\n",
       "  u'rojan'],\n",
       " [u'sea bass', u'seabass', u'bass'],\n",
       " [u'sea bream', u'bream', u'seabream'],\n",
       " [u'shao x', u'shao', u'shaox'],\n",
       " [u'shark fin', u'shark', u'sharkfin'],\n",
       " [u'shio yaki', u'shioyaki', u'shio', u'shiokyaki', u'shioyakki'],\n",
       " [u'shoe string', u'shoestr', u'shoestring', u'shoe'],\n",
       " [u'side winder', u'sidewinder', u'winder'],\n",
       " [u'skuttle butt', u'skuttle', u'skuttlebutt'],\n",
       " [u'soon dubu', u'dubu', u'soondubu'],\n",
       " [u'tar tar', u'tartar', u'tartare', u'tar', u'tartart', u'tatar'],\n",
       " [u'tenshin han', u'tenshinhan', u'tenshin'],\n",
       " [u'ting zai', u'ting', u'tingzai'],\n",
       " [u'tori soboro', u'soboro', u'torisoboro'],\n",
       " [u'yu sheng', u'sheng', u'yusheng'],\n",
       " [u'zha jiang', u'zhen jiang', u'jiang', u'zhajiang', u'zhenjiang']]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "checked = [[u'age dashi', u'agedashi',  u'dashitofu'],\n",
    " [u'back rib',  u'backrib', u'backribs'],\n",
    " [u'bean curd', u'beancurd', ],\n",
    " [u'bee hoon',  u'beehoon', u'meehoon'],\n",
    " [u'bi bim', u'bibim'],\n",
    " [u'chirashi don',  u'chirashidon'],\n",
    " [u'clay pot', u'claypot'],\n",
    " [u'cous cous', u'couscous'],\n",
    " [u'criss cut',  u'crisscut'],\n",
    " [u'dim sum',  u'dimsum'],\n",
    " [u'fai dang',u'faidang'],\n",
    " [u'fu rong', u'fu yong',  u'fuyong', u'furong'],\n",
    " [u'fugu mirin', u'fugumirin'],\n",
    " [u'hara bhara', u'harabhara',],\n",
    " [u'hor fun', u'horfun'],\n",
    " [u'ikan bili', u'ikambili', u'ikanbili'],\n",
    " [u'jing du', u'jingdu'],\n",
    " [u'kai lan',  u'kailan'],\n",
    " [u'kang kong', u'kangkong', u'kangkung'],\n",
    " [u'kra tiem',  u'kratiem', u'kratiam', u'takkrai'],\n",
    " [u'lily bulb', u'lilybulb'],\n",
    " [u'mc laren',  u'clare', u'mclaren', u'mclren'],\n",
    " [u'mentai yaki', u'mentaiyaki'],\n",
    " [u'nor mai', u'normai'],\n",
    " [u'phu nim', u'phunim'],\n",
    " [u'rogan josh', u'roganjosh',],\n",
    " [u'sea bass', u'seabass', ],\n",
    " [u'sea bream', u'seabream'],\n",
    " [u'shao x', u'shaox'],\n",
    " [u'shark fin',  u'sharkfin'],\n",
    " [u'shio yaki', u'shioyaki',  u'shiokyaki', u'shioyakki'],\n",
    " [u'shoe string', u'shoestr', u'shoestring'],\n",
    " [u'side winder', u'sidewinder',],\n",
    " [u'skuttle butt', u'skuttlebutt'],\n",
    " [u'soon dubu',  u'soondubu'],\n",
    " [u'tar tar', u'tartar', u'tartare', u'tartart', u'tatar'],\n",
    " [u'tenshin han', u'tenshinhan',],\n",
    " [u'ting zai',  u'tingzai'],\n",
    " [u'tori soboro',  u'torisoboro'],\n",
    " [u'yu sheng',  u'yusheng'],\n",
    " [u'zha jiang', u'zhajiang']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print len(check)==len(checked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of syn-token sets change from 1208 to 1209\n",
      "number of syn-token sets change from 1209 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n",
      "number of syn-token sets change from 1168 to 1168\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(check)):\n",
    "    syn_tokens = replace_syntoken_set(check[i], checked[i], syn_tokens)\n",
    "    syn_tokens = add_new([], vocab_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of syn-token sets change from 1168 to 1177\n"
     ]
    }
   ],
   "source": [
    "increased = [\n",
    "    [u'curd', u'curb'],   \n",
    "    [u'bi',u'bim'],  \n",
    "    [u'criss', u'cross', u'cris'],\n",
    "    [u'kang', u'kan'],\n",
    "    [u'zhen jiang', u'zhenjiang'],\n",
    "    [u'chirashi', u'shirashi'],\n",
    "    [u'rogan', u'roghan', u'rugan', u'rogam', u'rojan'],\n",
    "    [u'kra', u'krai'],\n",
    "    [u'bara',  u'bhara',  u'bharaa'],   \n",
    "]\n",
    "syn_tokens = add_new(increased, vocab_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: syn_token__20180207.p\n"
     ]
    }
   ],
   "source": [
    "save_syn_tokens(syn_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Substitute food names with predefined synonym tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 95613 unique non-drink/dessert food \n"
     ]
    }
   ],
   "source": [
    "print (\"Got %d unique non-drink/dessert food \" % \n",
    "       df[\"lemma_name\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrived: syn_token__20180207.p\n",
      "number of syn tokens: 1177\n"
     ]
    }
   ],
   "source": [
    "def retrive_syn_tokens():\n",
    "    import os\n",
    "    import pickle\n",
    "    prefix = \"syn_token\"    \n",
    "    date_list = list(set([f.split(\"__\")[-1].replace(\".p\", \"\") \n",
    "         for f in os.listdir(os.getcwd()) if (f.endswith(\".p\") and (prefix) in f)]))\n",
    "    recent_date = sorted(date_list)[-1]\n",
    "\n",
    "    file_name = \"__\".join([prefix, recent_date]) + \".p\"\n",
    "    with open(file_name, 'rb') as pfile:\n",
    "        retrived = pickle.load(pfile)       \n",
    "    print (\"retrived: %s\" % file_name)\n",
    "    print \"number of syn tokens: %d\"%len(retrived)\n",
    "    return retrived\n",
    "syn_token = retrive_syn_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rev_dict(x):\n",
    "    \"\"\" return the first element in lst as value \"\"\"\n",
    "    return dict((x[i],x[0]) for i in range(1, len(x)))\n",
    "def merge_dicts(d_lst):\n",
    "    \"\"\" shallow copy and merge into new dict\"\"\"\n",
    "    result = {}\n",
    "    for dictionary in d_lst:\n",
    "        result.update(dictionary)\n",
    "    return result\n",
    "\n",
    "def replace_token(s):\n",
    "    \"\"\" partial string matching \"\"\"\n",
    "    s = \" \" + s + \" \"\n",
    "    s = s.replace(\" \", \"  \")\n",
    "    for key in keys:       \n",
    "        s = s.replace(\" \" + key.replace(\" \", \"  \") + \" \", \" \" + rev_lookup[key] + \" \")\n",
    "    import re\n",
    "    s = re.sub(' +',' ', s.strip()) # multiple spaces    \n",
    "    return s    \n",
    "\n",
    "rev_lookup = merge_dicts([rev_dict(x) for x in syn_tokens])  \n",
    "keys = sorted(list(set(rev_lookup.keys())),key=lambda x: -len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10min 53s, sys: 5.45 s, total: 10min 58s\n",
      "Wall time: 10min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df[\"standard_name\"] = df[\"lemma_name\"].apply(replace_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: all_food_vendor__standard__20180207.p\n"
     ]
    }
   ],
   "source": [
    "file_name = \"__\".join([\"all_food_vendor\",\"standard\", f.today()]) + \".p\"\n",
    "f.save_file(df, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>food_name</th>\n",
       "      <th>vendor_name</th>\n",
       "      <th>locs</th>\n",
       "      <th>clean_name</th>\n",
       "      <th>lemma_name</th>\n",
       "      <th>lemma_name_2</th>\n",
       "      <th>standard_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\u0003Japanese Pickle Roll (6 pcs)</td>\n",
       "      <td>Sen of Japan</td>\n",
       "      <td>[(2017-07-03T17:55:01.668428, deliveroo/food/7...</td>\n",
       "      <td>japanese pickle roll</td>\n",
       "      <td>japanese pickle roll</td>\n",
       "      <td>japanese pickle roll</td>\n",
       "      <td>japanese pickle roll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\\t HK Kailan with Oyster Sauce\\t</td>\n",
       "      <td>Wee Nam Kee Hainanese Chicken Rice - Boon Lay</td>\n",
       "      <td>[(2018-01-01T04:00:05.703635, deliveroo/food/A...</td>\n",
       "      <td>kailan with oyster sauce</td>\n",
       "      <td>kailan with oyster sauce</td>\n",
       "      <td>kailan with oyster sauce</td>\n",
       "      <td>kai lan with oyster sauce</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          food_name  \\\n",
       "4     \u0003Japanese Pickle Roll (6 pcs)   \n",
       "5  \\t HK Kailan with Oyster Sauce\\t   \n",
       "\n",
       "                                     vendor_name  \\\n",
       "4                                   Sen of Japan   \n",
       "5  Wee Nam Kee Hainanese Chicken Rice - Boon Lay   \n",
       "\n",
       "                                                locs  \\\n",
       "4  [(2017-07-03T17:55:01.668428, deliveroo/food/7...   \n",
       "5  [(2018-01-01T04:00:05.703635, deliveroo/food/A...   \n",
       "\n",
       "                 clean_name                lemma_name  \\\n",
       "4      japanese pickle roll      japanese pickle roll   \n",
       "5  kailan with oyster sauce  kailan with oyster sauce   \n",
       "\n",
       "               lemma_name_2              standard_name  \n",
       "4      japanese pickle roll       japanese pickle roll  \n",
       "5  kailan with oyster sauce  kai lan with oyster sauce  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
