{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrive ES data for Bruppleinitial and Burpple\n",
    "    save the object with pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    >'burpple__review__unique__***today***.p',   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Conenct with ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system_info: 2.7.14 |Anaconda custom (64-bit)| (default, Oct 27 2017, 18:21:12) \n",
      "[GCC 7.2.0]\n",
      "path_info: /home/yueliu/Desktop/workspace_yue\n",
      "es_info: http://10.0.109.54:9200\n"
     ]
    }
   ],
   "source": [
    "# set up ElasticSearch object and the URL to access it\n",
    "def setup_es(isServer):\n",
    "    # python version\n",
    "    import sys\n",
    "    print \"system_info: %s\"%sys.version\n",
    "    # current working directory\n",
    "    import os\n",
    "    print \"path_info: %s\"%os.getcwd()    \n",
    "    ## Local on PC/laptop or on VM (10.0.106.122:2)   \n",
    "    from elasticsearch import Elasticsearch\n",
    "    port = \"9200\"\n",
    "    host = \"localhost\"\n",
    "    if isServer:\n",
    "        host = \"10.0.109.54\"    \n",
    "    url = \"http://\" + host + \":\" + port    \n",
    "    print \"es_info: %s\"%url    \n",
    "    es = Elasticsearch([{'host': host, 'port': port}])\n",
    "    return es, url\n",
    "\n",
    "es, url = setup_es(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\" : \"star7\",\n",
      "  \"cluster_name\" : \"star\",\n",
      "  \"cluster_uuid\" : \"VDcV69ThThyb00sd6PgeMA\",\n",
      "  \"version\" : {\n",
      "    \"number\" : \"2.4.5\",\n",
      "    \"build_hash\" : \"c849dd13904f53e63e88efc33b2ceeda0b6a1276\",\n",
      "    \"build_timestamp\" : \"2017-04-24T16:18:17Z\",\n",
      "    \"build_snapshot\" : false,\n",
      "    \"lucene_version\" : \"5.5.4\"\n",
      "  },\n",
      "  \"tagline\" : \"You Know, for Search\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# make sure ES is up and running\n",
    "import requests\n",
    "def initialise_es(i):\n",
    "    res = requests.get(url)\n",
    "    if i:\n",
    "        print(res.content)\n",
    "initialise_es(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define dict for b and bi\n",
    "burpple = {\"_index\":\"burpple\"}\n",
    "burppleinitial = {\"_index\":\"burppleinitial\"}\n",
    "burpples = [burpple, burppleinitial]\n",
    "for website in burpples:\n",
    "    website.update({\"vendor\":\"restaurant\",\n",
    "                    \"review\":\"review\",  \n",
    "                    \"user\":\"user\",                                   \n",
    "                    \"_id\":\"_id\",\n",
    "                    \"vendor_id\":'_source.id',\n",
    "                    \"vendor_url\":'_source.url',\n",
    "                    \"vendor_name\":\"_source.name\", \n",
    "                    \"address\":\"_source.address\", \n",
    "                    \"cuisine_tags\":\"_source.tags\", \n",
    "                    \"phone\":\"_source.phone\",                \n",
    "                    \"cycleStart\":'_source.crawlTimeStamp'})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "burpple.update({\"reviewFeedTime\":'_source.feedDatetime',})\n",
    "burppleinitial.update({\"reviewFeedTime\":'_source.datetime',})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Actual data retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def retrive_data (website, doc_type):\n",
    "    # activate ES\n",
    "    initialise_es(0)\n",
    "    # ES search pattern\n",
    "    _body = {\"query\": {\"match_all\": {}}}\n",
    "    _index = website['_index']\n",
    "    _doc_type = website[doc_type]\n",
    "\n",
    "    # With the help of a generator, get all records\n",
    "    from elasticsearch import helpers\n",
    "    scanResp = helpers.scan(es, _body, scroll= \"2m\", \n",
    "                            index= _index, \n",
    "                            doc_type= _doc_type, \n",
    "                            timeout=\"2m\")\n",
    "    recs = []\n",
    "    for rec in scanResp:\n",
    "        recs.append(rec)\n",
    "    print len(recs)\n",
    "\n",
    "    # Convert unicode to string (ascii, ignore unicode such as '\\xae')\n",
    "    def convert_unicode(data):\n",
    "        if isinstance(data, basestring):\n",
    "            return (data.encode(\"ascii\",\"ignore\"))\n",
    "        elif isinstance(data, collections.Mapping):\n",
    "            return dict(map(convert_unicode, data.iteritems()))\n",
    "        elif isinstance(data, collections.Iterable):\n",
    "            return type(data)(map(convert_unicode, data))\n",
    "        else:\n",
    "            return data\n",
    "\n",
    "    # json file to dataframe \n",
    "    import collections\n",
    "    import pandas as pd\n",
    "    pd.options.mode.chained_assignment = None # default is warn\n",
    "    from pandas.io.json import json_normalize\n",
    "    lst_rec = []\n",
    "    for line in recs:\n",
    "        line = convert_unicode(line)\n",
    "        lst_rec.append(json_normalize(line))    \n",
    "    df = pd.concat(lst_rec) \n",
    "    # Print shape and all attributes\n",
    "    print (\"ES location: %s, %s\"%(_index,website[doc_type]))\n",
    "    print(\"Dimention: %d , %d\"%df.shape)\n",
    "    print(\"Column names::%s\"%\", \".join(df.columns.tolist()))\n",
    "    print (\"\\n\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\" : \"star7\",\n",
      "  \"cluster_name\" : \"star\",\n",
      "  \"cluster_uuid\" : \"VDcV69ThThyb00sd6PgeMA\",\n",
      "  \"version\" : {\n",
      "    \"number\" : \"2.4.5\",\n",
      "    \"build_hash\" : \"c849dd13904f53e63e88efc33b2ceeda0b6a1276\",\n",
      "    \"build_timestamp\" : \"2017-04-24T16:18:17Z\",\n",
      "    \"build_snapshot\" : false,\n",
      "    \"lucene_version\" : \"5.5.4\"\n",
      "  },\n",
      "  \"tagline\" : \"You Know, for Search\"\n",
      "}\n",
      "\n",
      "\n",
      "25709\n",
      "ES location: burpple, review\n",
      "Dimention: 25709 , 43\n",
      "Column names::_id, _index, _score, _source.body, _source.crawlTimeStamp, _source.feedDatetime, _source.foodImgUrl, _source.id, _source.name, _source.postDatetime, _source.restaurant, _source.restaurant.address, _source.restaurant.areaUrls, _source.restaurant.avgPrice, _source.restaurant.crawlTimeStamp, _source.restaurant.id, _source.restaurant.name, _source.restaurant.openingHours, _source.restaurant.phone, _source.restaurant.tags, _source.restaurant.url, _source.restaurant.website, _source.restaurantUrl, _source.title, _source.url, _source.user, _source.user.awards, _source.user.badges, _source.user.bio, _source.user.country, _source.user.crawlTimeStamp, _source.user.name, _source.user.numOfFollowers, _source.user.numOfFollowing, _source.user.numOfReviews, _source.user.numOfWishlists, _source.user.url, _source.user.userImgUrl, _source.user.username, _source.user.website, _source.userImgUrl, _source.username, _type\n",
      "\n",
      "\n",
      "CPU times: user 2min 42s, sys: 5.7 s, total: 2min 48s\n",
      "Wall time: 2min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "initialise_es(1)\n",
    "print \"\"\n",
    "\n",
    "website = burpple\n",
    "website.update({\"all_rec_review\" : retrive_data (website, \"review\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297480\n",
      "ES location: burppleinitial, review\n",
      "Dimention: 297480 , 44\n",
      "Column names::_id, _index, _score, _source.areaName, _source.body, _source.crawlTimeStamp, _source.datetime, _source.foodImgUrl, _source.id, _source.name, _source.offset, _source.restaurant, _source.restaurant.address, _source.restaurant.areaUrls, _source.restaurant.avgPrice, _source.restaurant.crawlTimeStamp, _source.restaurant.id, _source.restaurant.name, _source.restaurant.openingHours, _source.restaurant.phone, _source.restaurant.tags, _source.restaurant.url, _source.restaurant.website, _source.restaurantUrl, _source.title, _source.url, _source.user, _source.user.awards, _source.user.badges, _source.user.bio, _source.user.country, _source.user.crawlTimeStamp, _source.user.name, _source.user.numOfFollowers, _source.user.numOfFollowing, _source.user.numOfReviews, _source.user.numOfWishlists, _source.user.url, _source.user.userImgUrl, _source.user.username, _source.user.website, _source.userImgUrl, _source.username, _type\n",
      "\n",
      "\n",
      "CPU times: user 31min 31s, sys: 36.4 s, total: 32min 8s\n",
      "Wall time: 32min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "website = burppleinitial\n",
    "website.update({\"all_rec_review\" : retrive_data (website, \"review\")})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Duplicate records removal  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_bu_dup_records(burpples, non_sg_vendors):\n",
    "    # not included: '_source.url', # https://www.burpple.com/f/ + \"_id\"\n",
    "    cols = ['_id', # review identifier, something like \"liKrL-pE\"\n",
    "            '_type',\n",
    "            '_index', # burpple / burpple initial\n",
    "            '_source.title', # title of review, with some special characters\n",
    "            '_source.body', # text \n",
    "            '_source.crawlTimeStamp', \n",
    "            '_source.foodImgUrl',             \n",
    "            '_source.username', # user identifier\n",
    "            '_source.restaurant.id',\n",
    "            '_source.restaurant.name'] #vendor identifier\n",
    "    # merge feed time\n",
    "    import pandas as pd\n",
    "    pd.options.mode.chained_assignment = None # default is warn\n",
    "    df = pd.concat([site[\"all_rec_review\"]\n",
    "                    [cols+ [site[\"reviewFeedTime\"]]] for site in burpples])\n",
    "    df[\"feedTime\"] = df[[site[\"reviewFeedTime\"] for site in burpples]].fillna('').sum(axis=1)\n",
    "    df = df[cols +[\"feedTime\"]]\n",
    "    # sort, only leaving the latest crawled first\n",
    "    df = df.sort_values(by=['_source.crawlTimeStamp',\"_id\"], ascending=[False,True])\n",
    "    df = df.groupby(\"_id\").first()\n",
    "    df.reset_index(inplace=True)\n",
    "    # remove reviews of non-sg vendors\n",
    "    df = df[~df[\"_source.restaurant.id\"].isin(non_sg_vendors)]\n",
    "    \n",
    "    print (\"Got %d unique records of review\" % len(df))\n",
    "    print (\"Got %d unique records of images\" % \n",
    "           df[df['_source.foodImgUrl']!=\"\"]['_source.foodImgUrl'].nunique())\n",
    "    print (\"Got %d unique users\" % df['_source.username'].nunique())\n",
    "    print (\"Got %d unique vendors\" % df['_source.restaurant.id'].nunique())\n",
    "    print (\"Review time from %s to %s\" % (df['feedTime'].min(),df['feedTime'].max()))\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 323104 unique records of review\n",
      "Got 323051 unique records of images\n",
      "Got 11180 unique users\n",
      "Got 14547 unique vendors\n",
      "Review time from 2011-11-08 00:00:00.000000 to 2017-07-20 23:04:45.837391\n",
      "CPU times: user 8.96 s, sys: 764 ms, total: 9.73 s\n",
      "Wall time: 9.73 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "non_sg_vendors = ['114803', '119954', '136868', '139058', '149618', '155202', '156512', '16431',\n",
    "                  '165933', '166041', '174029', '28318', '43464', '51131', '59732', '63212']\n",
    "df = remove_bu_dup_records(burpples, non_sg_vendors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>_type</th>\n",
       "      <th>_index</th>\n",
       "      <th>_source.title</th>\n",
       "      <th>_source.body</th>\n",
       "      <th>_source.crawlTimeStamp</th>\n",
       "      <th>_source.foodImgUrl</th>\n",
       "      <th>_source.username</th>\n",
       "      <th>_source.restaurant.id</th>\n",
       "      <th>_source.restaurant.name</th>\n",
       "      <th>feedTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--0EG2cp</td>\n",
       "      <td>review</td>\n",
       "      <td>burppleinitial</td>\n",
       "      <td>Sicilian Lemon Creme andCaramelized Red Mixed ...</td>\n",
       "      <td></td>\n",
       "      <td>2017-04-03 11:00:52.282519</td>\n",
       "      <td>https://s3.burpple.com/foods/376d87a944deb0499...</td>\n",
       "      <td>@ieatkows</td>\n",
       "      <td>34166</td>\n",
       "      <td>OSO Ristorante</td>\n",
       "      <td>2014-12-22 00:00:00.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>--0ck5YP</td>\n",
       "      <td>review</td>\n",
       "      <td>burppleinitial</td>\n",
       "      <td>Just look at their soy sauce chicken.</td>\n",
       "      <td>The tender and juicy thigh meat with their fra...</td>\n",
       "      <td>2017-04-03 11:00:52.282519</td>\n",
       "      <td>https://s3.burpple.com/foods/2fbab5e890cf97f31...</td>\n",
       "      <td>@jiaknonstop</td>\n",
       "      <td>164762</td>\n",
       "      <td>Fatty Ox Hong Kong Kitchen (Chinatown Complex ...</td>\n",
       "      <td>2014-12-15 00:00:00.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>--0gMGRO</td>\n",
       "      <td>review</td>\n",
       "      <td>burppleinitial</td>\n",
       "      <td>Holy, smokin' Burnt Corn Tacos - it's their WE...</td>\n",
       "      <td>(Read: time for MORE alcohol.)  #AntiDoteBar #...</td>\n",
       "      <td>2017-04-03 11:00:52.282519</td>\n",
       "      <td>https://s3.burpple.com/foods/16ff778f20e5f8614...</td>\n",
       "      <td>@Jazpster</td>\n",
       "      <td>539</td>\n",
       "      <td>Asian Market Caf (Fairmont Singapore)</td>\n",
       "      <td>2016-11-18 00:00:00.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        _id   _type          _index  \\\n",
       "0  --0EG2cp  review  burppleinitial   \n",
       "1  --0ck5YP  review  burppleinitial   \n",
       "2  --0gMGRO  review  burppleinitial   \n",
       "\n",
       "                                       _source.title  \\\n",
       "0  Sicilian Lemon Creme andCaramelized Red Mixed ...   \n",
       "1              Just look at their soy sauce chicken.   \n",
       "2  Holy, smokin' Burnt Corn Tacos - it's their WE...   \n",
       "\n",
       "                                        _source.body  \\\n",
       "0                                                      \n",
       "1  The tender and juicy thigh meat with their fra...   \n",
       "2  (Read: time for MORE alcohol.)  #AntiDoteBar #...   \n",
       "\n",
       "       _source.crawlTimeStamp  \\\n",
       "0  2017-04-03 11:00:52.282519   \n",
       "1  2017-04-03 11:00:52.282519   \n",
       "2  2017-04-03 11:00:52.282519   \n",
       "\n",
       "                                  _source.foodImgUrl _source.username  \\\n",
       "0  https://s3.burpple.com/foods/376d87a944deb0499...        @ieatkows   \n",
       "1  https://s3.burpple.com/foods/2fbab5e890cf97f31...     @jiaknonstop   \n",
       "2  https://s3.burpple.com/foods/16ff778f20e5f8614...        @Jazpster   \n",
       "\n",
       "  _source.restaurant.id                            _source.restaurant.name  \\\n",
       "0                 34166                                     OSO Ristorante   \n",
       "1                164762  Fatty Ox Hong Kong Kitchen (Chinatown Complex ...   \n",
       "2                   539              Asian Market Caf (Fairmont Singapore)   \n",
       "\n",
       "                     feedTime  \n",
       "0  2014-12-22 00:00:00.000000  \n",
       "1  2014-12-15 00:00:00.000000  \n",
       "2  2016-11-18 00:00:00.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_files(df, doc_type):\n",
    "    # data description as file name\n",
    "    import datetime\n",
    "    today = datetime.date.today()\n",
    "    today = today.strftime('%Y%m%d')\n",
    "    desc = \"__\".join(doc_type + [today])  \n",
    "    file_name = desc + \".p\"\n",
    "    print file_name\n",
    "    # save the fil`e as pickle file\n",
    "    import pickle\n",
    "    with open(file_name, 'wb') as pfile:\n",
    "        pickle.dump(df, pfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "burpple__review__unique__20180202.p\n",
      "CPU times: user 18.7 s, sys: 1.16 s, total: 19.9 s\n",
      "Wall time: 19.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc_type = [\"burpple\", \"review\", \"unique\"]\n",
    "save_files(df, doc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# retrive for testing\n",
    "def retrive_file(file_name):\n",
    "    import pickle\n",
    "    with open(file_name, 'rb') as pfile:\n",
    "        retrived = pickle.load(pfile)\n",
    "    return retrived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_name = \"burpple__review__unique__20180202.p\"    \n",
    "df_retrived = retrive_file(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Obtain review objects with clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None # default is warn\n",
    "# new col: loc\n",
    "df[\"loc\"] = df[\"_index\"] + \"/\" + df[\"_type\"] + \"/\" + df[\"_id\"] \n",
    "cols = ['loc','_source.title','_source.body','_source.restaurant.name']\n",
    "df = df[cols]\n",
    "rename_cols = [col.replace(\"_source.\", \"\") for col in cols]\n",
    "df.columns = rename_cols\n",
    "# new col: full_review\n",
    "df[\"full_review\"] = df['title'] + \" \" + df['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "twitter style text-cleaning, for url, mention, hashtag, \n",
    "also: dollar, score\n",
    "not: apostrophe conversion, stop words, emoticons, slang, word standardization\n",
    "\"\"\"\n",
    "def clean_review(s):\n",
    "    import re   \n",
    "    # 1. etract urls (may contain $, #, @), remove url part from s\n",
    "    p = r'http[s]?://(?:[a-z]|[0-9]|[$-_@#.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+'\n",
    "    urls = tuple(re.findall(p, s))\n",
    "    for rep in urls:\n",
    "        s = s.replace(rep,\"\")\n",
    "    # 2. normal cleaning: replace w/ and &\n",
    "    s = \" \" + s +\" \"\n",
    "    s = s.replace(\"w/\", \" with \")\n",
    "    s = s.replace(\"&\", \" and \") \n",
    "    # 3. textract hashtags \n",
    "    hashtags = tuple(re.findall(r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", s))\n",
    "    # 4. extract mentions\n",
    "    mentions = tuple(re.findall(r'(?:@[\\w_]+)', s))   \n",
    "    # 5. extract dolalr amounts\n",
    "    dollars = tuple([x[0] for x in re.findall(r'(\\$\\d+([,\\.]\\d+)?(\\+\\+)?(\\+)?k?)', s)])\n",
    "    # 6. extract scores\n",
    "    scores = tuple([x[0] for x in re.findall(r'(\\d+([\\.]\\d+)(\\/)\\d+([\\.]\\d+)?)', s)])\n",
    "    # 7. remove above parts from review, strip and remove multiple space\n",
    "    words = sorted(list(dollars + scores + hashtags + mentions), key=len, reverse=True)\n",
    "    for rep in words:\n",
    "        s = s.replace(rep,\"\")\n",
    "    s = re.sub(' +',' ', s.strip()) \n",
    "    lst = [s, hashtags, mentions, dollars, scores, urls]\n",
    "    return (tuple(lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.7 s, sys: 460 ms, total: 16.2 s\n",
      "Wall time: 16.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df[\"review_text\"], df[\"hashtags\"], df[\"mentions\"], df[\"dollars\"], df[\"scores\"], df[\"urls\"] = \\\n",
    "zip(*df[\"full_review\"].apply(clean_review))\n",
    "cols = df.columns.tolist()\n",
    "cols.remove(\"full_review\")\n",
    "df = df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(323104, 10)\n",
      "['loc', 'title', 'body', 'restaurant.name', 'review_text', 'hashtags', 'mentions', 'dollars', 'scores', 'urls']\n"
     ]
    }
   ],
   "source": [
    "print df.shape\n",
    "print df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loc</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>restaurant.name</th>\n",
       "      <th>review_text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>mentions</th>\n",
       "      <th>dollars</th>\n",
       "      <th>scores</th>\n",
       "      <th>urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>burppleinitial/review/--0EG2cp</td>\n",
       "      <td>Sicilian Lemon Creme andCaramelized Red Mixed ...</td>\n",
       "      <td></td>\n",
       "      <td>OSO Ristorante</td>\n",
       "      <td>Sicilian Lemon Creme andCaramelized Red Mixed ...</td>\n",
       "      <td>(#igsg, #igers, #igfood, #yum, #yummy, #instaf...</td>\n",
       "      <td>()</td>\n",
       "      <td>()</td>\n",
       "      <td>()</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>burppleinitial/review/--0ck5YP</td>\n",
       "      <td>Just look at their soy sauce chicken.</td>\n",
       "      <td>The tender and juicy thigh meat with their fra...</td>\n",
       "      <td>Fatty Ox Hong Kong Kitchen (Chinatown Complex ...</td>\n",
       "      <td>Just look at their soy sauce chicken. The tend...</td>\n",
       "      <td>()</td>\n",
       "      <td>()</td>\n",
       "      <td>()</td>\n",
       "      <td>()</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>burppleinitial/review/--0gMGRO</td>\n",
       "      <td>Holy, smokin' Burnt Corn Tacos - it's their WE...</td>\n",
       "      <td>(Read: time for MORE alcohol.)  #AntiDoteBar #...</td>\n",
       "      <td>Asian Market Caf (Fairmont Singapore)</td>\n",
       "      <td>Holy, smokin' Burnt Corn Tacos - it's their WE...</td>\n",
       "      <td>(#AntiDoteBar, #FairmontSingapore)</td>\n",
       "      <td>()</td>\n",
       "      <td>()</td>\n",
       "      <td>()</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              loc  \\\n",
       "0  burppleinitial/review/--0EG2cp   \n",
       "1  burppleinitial/review/--0ck5YP   \n",
       "2  burppleinitial/review/--0gMGRO   \n",
       "\n",
       "                                               title  \\\n",
       "0  Sicilian Lemon Creme andCaramelized Red Mixed ...   \n",
       "1              Just look at their soy sauce chicken.   \n",
       "2  Holy, smokin' Burnt Corn Tacos - it's their WE...   \n",
       "\n",
       "                                                body  \\\n",
       "0                                                      \n",
       "1  The tender and juicy thigh meat with their fra...   \n",
       "2  (Read: time for MORE alcohol.)  #AntiDoteBar #...   \n",
       "\n",
       "                                     restaurant.name  \\\n",
       "0                                     OSO Ristorante   \n",
       "1  Fatty Ox Hong Kong Kitchen (Chinatown Complex ...   \n",
       "2              Asian Market Caf (Fairmont Singapore)   \n",
       "\n",
       "                                         review_text  \\\n",
       "0  Sicilian Lemon Creme andCaramelized Red Mixed ...   \n",
       "1  Just look at their soy sauce chicken. The tend...   \n",
       "2  Holy, smokin' Burnt Corn Tacos - it's their WE...   \n",
       "\n",
       "                                            hashtags mentions dollars scores  \\\n",
       "0  (#igsg, #igers, #igfood, #yum, #yummy, #instaf...       ()      ()     ()   \n",
       "1                                                 ()       ()      ()     ()   \n",
       "2                 (#AntiDoteBar, #FairmontSingapore)       ()      ()     ()   \n",
       "\n",
       "  urls  \n",
       "0   ()  \n",
       "1   ()  \n",
       "2   ()  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_files(df, doc_type):\n",
    "    # data description as file name\n",
    "    import datetime\n",
    "    today = datetime.date.today()\n",
    "    today = today.strftime('%Y%m%d')\n",
    "    desc = \"__\".join(doc_type+[today])  \n",
    "    file_name = desc + \".p\"\n",
    "    print file_name\n",
    "    # save the fil`e as pickle file\n",
    "    import pickle\n",
    "    with open(file_name, 'wb') as pfile:\n",
    "        pickle.dump(df, pfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "burpple__review__clean__20180205.p\n",
      "CPU times: user 17.5 s, sys: 464 ms, total: 18 s\n",
      "Wall time: 18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc_type = [\"burpple\", \"review\", \"clean\"]\n",
    "save_files(df, doc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.6 s, sys: 300 ms, total: 17.9 s\n",
      "Wall time: 17.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "file_name = \"burpple__review__clean__20180205.p\"    \n",
    "df_retrived = retrive_file(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
